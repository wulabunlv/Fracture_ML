{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Feb  2 13:04:21 2019\n",
    "\n",
    "@author:\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from matplotlib import pyplot\n",
    "import pandas\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "NUMERICALS = ['GIAGE1', 'HA_HEIGHT', 'HA_WEIGHT', 'TUDRPRWK', 'B1FND', 'HA_SMOKE_0.0', 'HA_SMOKE_1.0', 'HA_SMOKE_2.0', 'GIERACE_1.0',\n",
    "              'GIERACE_2.0', 'GIERACE_3.0', 'GIERACE_4.0', 'GIERACE_5.0', 'CLINIC_1.0','CLINIC_2.0', 'CLINIC_3.0',  \n",
    "              'CLINIC_4.0', 'CLINIC_5.0', 'CLINIC_6.0','NFWLKSPD_0.0', 'NFWLKSPD_1.0', 'NFWLKSPD_2.0', 'BUAMEAN', 'QUIMEAN', 'SOSMEAN', 'PHYS_MROS_1.0', 'PHYS_MROS_2.0', 'PHYS_MROS_3.0']\n",
    "\n",
    "\n",
    "# change allele to number\n",
    "def allele_to_number(sample, attribute, sig_dict):\n",
    "    return sample[attribute].count(sig_dict[attribute])\n",
    "\n",
    "\n",
    "# fill numerical empty cells with median of the column, race with 1 (for white) and other categorical empty cells with the 0\n",
    "def fill_empty_cell(sample, attribute, data):\n",
    "    if pandas.isnull(sample[attribute]):\n",
    "        if attribute in NUMERICALS:\n",
    "            return data[attribute].median()\n",
    "        return int(attribute == 'GIERACE' )\n",
    "    elif attribute == 'FRAC':\n",
    "        return int(sample['FAANYSLD'] or sample['FAANYWST'] or sample['FAANYHIP'] or sample['XMDSQGE1'])\n",
    "    return sample[attribute]\n",
    "\n",
    "\n",
    "# calculate GRS\n",
    "def load_weight(sheet):\n",
    "    weight = []\n",
    "    df = pandas.read_excel('Estrada_63.xlsx', sheet_name=sheet)\n",
    "    w = list(df)[-1]\n",
    "    for i in range(len(df)):\n",
    "        weight.append(df.iloc[i][w])\n",
    "    return weight\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pandas.read_excel('mros_1103snps.xlsx')\n",
    "    # drop HA_SLDFXFU where only 10% is filled, drop subjectid,\n",
    "    data.drop(['HA_SLDFXFU', 'TURSMOKE', 'HA_SLDFX', 'HA_WRSTFX'], axis=1, inplace=True)\n",
    "    # make the fractures into 1 variable\n",
    "    data['FRAC'] = 0\n",
    "    for attribute in data.keys():\n",
    "        data[attribute] = data.apply(lambda sample: fill_empty_cell(sample, attribute, data), axis=1)\n",
    "    # drop the other fractured values\n",
    "    data.drop(['FAANYSLD', 'FAANYWST'], axis=1, inplace=True)\n",
    "    # encode the categorical data\n",
    "#     data = pandas.DataFrame(pandas.get_dummies(data, columns=['GIERACE', 'CLINIC', 'NFWLKSPD', 'HA_SMOKE', 'PHYS_MROS']))\n",
    "    # features = list(data)[22:-6]\n",
    "    # setting Y and X\n",
    "    # Y_df = np.asarray(data['B1THD'], dtype=\"|S8\")\n",
    "    Y_df = data['FRAC']\n",
    "    X_df = pandas.read_excel('ready_to_go.xlsx')\n",
    "    #X_df = data.drop(['SUBJECTID', 'HA_LSD', 'BUAMEAN', 'FAHIPFV1', 'FASLDFV1', 'FAWSTFV1', 'EFSTATUS',\n",
    "    #                    'XMDSQGE1', 'XMSQGE2',  'HA_BMI', 'FAANYHIP', 'HA_CALCIUM', 'FRAC', 'PHYS_MROS'], axis=1)\n",
    "    # weight_LS = load_weight('LS_sex-combined_beta')\n",
    "   # feature_data = data[features]\n",
    "    # weight_LS = pandas.DataFrame(pandas.Series(weight_LS, index=features, name=0))\n",
    "    #weight_FN = load_weight('FN_sex-combined_beta')\n",
    "    # X_df['GRS_LS'] = feature_data.dot(weight_LS)\n",
    "   # X_df['GRS_FN'] = feature_data.dot(weight_FN)\n",
    "   # X_df.drop(features, axis=1, inplace=True)\n",
    "    # print(Y_df.shape)\n",
    "    \n",
    "    # to export the fracture and subject ID\n",
    "#     export = data[['SUBJECTID', 'FRAC', 'SOSMEAN', 'B1FND', 'B1THD', 'B1TLD', 'W_SCORE', 'GIAGE1', 'HA_HEIGHT', 'HA_WEIGHT', 'TUDRPRWK', 'QLFXST51', 'NFWLKSPD', 'PHYS_MROS', 'GIERACE', 'HA_SMOKE', 'CLINIC']]\n",
    "#     export.to_csv('export.csv')\n",
    "#     data.to_csv('X_variables.csv')\n",
    "    \n",
    "#     Y_df.to_csv('MOF.csv')\n",
    "    \n",
    "    print(Y_df.head())\n",
    "    print(X_df[1:1])\n",
    "    print(X_df.shape)\n",
    "    print(Y_df.shape)\n",
    "\n",
    "    with open('datamros1103', 'wb') as data_file_handler:\n",
    "        import pickle\n",
    "\n",
    "        pickle.dump(\n",
    "            dict(\n",
    "                X=X_df,\n",
    "                Y=Y_df\n",
    "            ),\n",
    "            data_file_handler\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('mros_1103snps.xlsx')\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "data1 = data.fillna(\" \")\n",
    "\n",
    "data1.insull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y_df.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "cv = pandas.read_excel('contious_variable.xlsx')\n",
    "    \n",
    "mean_cv = numpy.mean(cv, axis=0)\n",
    "std_cv = numpy.std(cv, axis=0)\n",
    "stan_cv = (cv - mean_cv) / std_cv  # mean variance normalization\n",
    "    \n",
    "stan_cv.to_csv('norma_continu_var.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('datamros1103', 'rb') as file_handler:\n",
    "    data = pickle.load(file_handler)\n",
    "    X, Y = data.get('X', []).values, data.get('Y', []).values\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 6\n",
    "#\n",
    "# # The below is necessary for starting Numpy generated random numbers\n",
    "# # in a well-defined initial state.\n",
    "np.random.seed(seed)\n",
    "# # The below is necessary for starting core Python generated random numbers\n",
    "# # in a well-defined state.\n",
    "rn.seed(seed)\n",
    "\n",
    "# according to keras documentation, numpy seed should be set before importing keras\n",
    "# information regarding setup for obtaining reproducible results using Keras during development in the following link\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "#tf.set_random_seed(seed)\n",
    "# Y = label_binarize(Y, classes=[0,1])\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "\n",
    "optimizer = 'adamax'\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "number_of_data = X.shape[0]\n",
    "number_of_train_data = int(.8*number_of_data)\n",
    "number_of_test_data = number_of_data-number_of_train_data\n",
    "\n",
    "# load dataset\n",
    "x_train, x_test = X[:number_of_train_data, :], X[number_of_train_data:, :]\n",
    "#mean_train_data = np.mean(train_data, axis=0)\n",
    "#std_train_data = np.std(train_data, axis=0)\n",
    "#x_train = (train_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "#x_test = (test_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "y_train, y_test = Y[:number_of_train_data], Y[number_of_train_data:]\n",
    "#x_test, y_test = sm.fit_resample(X,Y)\n",
    "\n",
    "# X_df, Xtest, Y_df, ytest = train_test_split(X_df, Y_df, test_size=0.2)\n",
    "# Xtrain, ytrain = sm.fit_resample(X_df, Y_df)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "# y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# def create_model():\n",
    "#     model = LogisticRegression(solver='lbfgs',max_iter=35)\n",
    "#     return model;\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=26, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='sgd',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, brier_score_loss, precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve, accuracy_score\n",
    "from sklearn import metrics\n",
    "from inspect import signature\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state=2, ratio = 1.0)\n",
    "x_train_s, y_train_s = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "model.fit(x_train_s, y_train_s, epochs=1, batch_size=batch_size, verbose=1)\n",
    "y_pred = model.predict(x_test)\n",
    "print('The predicted y_value:', y_pred)\n",
    "# y_pred = model.predict(x_test)\n",
    "#y_pred = y_pred.astype('int32')\n",
    "#print(y_pred, ' YPOST')\n",
    "yscore_raw = model.predict_proba(x_test)\n",
    "yscore = [s[0] for s in yscore_raw]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yscore)\n",
    "#final = [(lambda i: 0 if i <= np.average(thresholds) else 1)(i) for i in y_pred]\n",
    "final = [(lambda i: 0 if i <= np.average(thresholds) else 1)(i) for i in y_pred]\n",
    "\n",
    "print(confusion_matrix(y_test, final))\n",
    "\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.to_csv('y_test.csv')\n",
    "\n",
    "y_pred = pd.Series(final)\n",
    "y_pred.to_csv('y_pred_frac_logi.csv')\n",
    "\n",
    "# np.savetxt('logi_yscore_raw.csv', yscore_raw, delimiter=',')\n",
    "\n",
    "# predict probabilities\n",
    "# probs = model.predict_proba(x_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "# probs = probs[:, 1]\n",
    "\n",
    "# predict class values\n",
    "# yhat = model.predict(x_test)\n",
    "\n",
    "# calculate precision-recall AUC\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "\n",
    "# calculate F1 score\n",
    "# f1 = f1_score(y_test, yhat, normalize = False)\n",
    "\n",
    "# calculate precision-recall AUC\n",
    "# auc = auc(recall, precision)\n",
    "\n",
    "# calculate average precision score\n",
    "\n",
    "# ap = average_precision_score(y_test, probs)\n",
    "# print('f1=%.3f auc=%.3f' % (f1, auc, ap))\n",
    "\n",
    "# plot no skill\n",
    "# plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "\n",
    "# plot the precision-recall curve for the model\n",
    "# plt.plot(recall, precision, marker='.')\n",
    "\n",
    "# show the plot\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#print(list(X))\n",
    "#print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "#print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "#print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "#print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "\n",
    "# average_precision = average_precision_score(y_test, yscore)\n",
    "\n",
    "# print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "# precision, recall, _ = precision_recall_curve(y_test, yscore)\n",
    "\n",
    "#step_kwargs = ({'step': 'post'}\n",
    "#              if 'step' in signature(plt.fill_between).parameters\n",
    "#              else {})\n",
    "\n",
    "# plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "# plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "# plt.savefig('logistic_Regression_PRCurve.png')\n",
    "# plt.show()   # Display\n",
    "\n",
    "# y_test =  pd.Series(y_test)\n",
    "# y_test.to_csv('y_test.csv')\n",
    "\n",
    "# logi_final = pd.Series(final)\n",
    "# logi_final.to_csv('logi_y_pred.csv')\n",
    "\n",
    "# Calculate the 95% CI for AUC in Logistic Regression\n",
    "def conf_auc(test_predictions, ground_truth, bootstrap=1000, seed=None,  confint=0.95):\n",
    "    \"\"\"Takes as input test predictions, ground truth, number of bootstraps, seed, and confidence interval\"\"\"\n",
    "    #inspired by https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals by ogrisel\n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(seed)\n",
    "    if confint>1:\n",
    "        confint=confint/100\n",
    "    for i in range(bootstrap):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(test_predictions) - 1, len(test_predictions))\n",
    "        if len(np.unique(ground_truth[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        score = metrics.roc_auc_score(ground_truth[indices], test_predictions[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    lower_bound=(1-confint)/2\n",
    "    upper_bound=1-lower_bound\n",
    "    confidence_lower = sorted_scores[int(lower_bound * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(upper_bound * len(sorted_scores))]\n",
    "    auc = metrics.roc_auc_score(ground_truth, test_predictions)\n",
    "    print(\"{:0.0f}% confidence interval for the score: [{:0.3f} - {:0.3}] and your AUC is: {:0.3f}\".format(confint*100, confidence_lower, confidence_upper, auc))\n",
    "    confidence_interval = (confidence_lower, auc, confidence_upper)\n",
    "    return confidence_interval\n",
    "\n",
    "conf_auc(y_pred, y_test)\n",
    "\n",
    "\n",
    "# Calculate the 95% CI for Accuracy in Logistic Regression\n",
    "def conf_accu(test_predictions, ground_truth, bootstrap=1000, seed=None,  confint=0.95):\n",
    "    \"\"\"Takes as input test predictions, ground truth, number of bootstraps, seed, and confidence interval\"\"\"\n",
    "    #inspired by https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals by ogrisel\n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(seed)\n",
    "    if confint>1:\n",
    "        confint=confint/100\n",
    "    for i in range(bootstrap):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(test_predictions) - 1, len(test_predictions))\n",
    "        if len(np.unique(ground_truth[indices])) < 2:\n",
    "            continue\n",
    "        \n",
    "        score = metrics.accuracy_score(ground_truth[indices], test_predictions[indices], normalize=False)\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    lower_bound=(1-confint)/2\n",
    "    upper_bound=1-lower_bound\n",
    "    confidence_lower = sorted_scores[int(lower_bound * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(upper_bound * len(sorted_scores))]\n",
    "    auc = metrics.accuracy_score(ground_truth, test_predictions.round(indices), normalize=False)\n",
    "    print(\"{:0.0f}% confidence interval for the score: [{:0.3f} - {:0.3}] and your accuracy is: {:0.3f}\".format(confint*100, confidence_lower, confidence_upper, auc))\n",
    "    confidence_interval = (confidence_lower, auc, confidence_upper)\n",
    "    return confidence_interval\n",
    "\n",
    "#conf_accu(y_pred, y_test)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, yscore)\n",
    "plt.plot(fpr, tpr, '^k:', label='%s ROC (area = %0.2f)' % ('Logistic Regression', auc))\n",
    "\n",
    "# Calculate Area under the curve to display on the plot\n",
    "\n",
    "# Now, plot the computed values\n",
    "\n",
    "# Custom settings for the plot\n",
    "# Receiver Operating Characteristic-\n",
    "plt.plot([0, 1], [0, 1], '-k')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Logistic Regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('logistic_Regression_MOF.png')\n",
    "plt.show()   # Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import scikitplot\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, brier_score_loss, precision_score, recall_score, f1_score, average_precision_score, precision_recall_curve, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold  # http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUMERICALS = ['GIAGE1', 'HA_HEIGHT', 'HA_WEIGHT', 'TUDRPRWK', 'B1FND', 'HA_SMOKE_0.0', 'HA_SMOKE_1.0', 'HA_SMOKE_2.0', 'GIERACE_1.0',\n",
    "              'GIERACE_2.0', 'GIERACE_3.0', 'GIERACE_4.0', 'GIERACE_5.0', 'CLINIC_1.0','CLINIC_2.0', 'CLINIC_3.0',  \n",
    "              'CLINIC_4.0', 'CLINIC_5.0', 'CLINIC_6.0','NFWLKSPD_0.0', 'NFWLKSPD_1.0', 'NFWLKSPD_2.0', 'BUAMEAN', 'QUIMEAN', 'SOSMEAN', 'PHYS_MROS_1.0', 'PHYS_MROS_2.0', 'PHYS_MROS_3.0']\n",
    "\n",
    "RANDOM_STATE = 43\n",
    "\n",
    "# fill numerical empty cells with median of the column, race with 1 (for white) and other categorical empty cells with the 0\n",
    "def fill_empty_cell(sample, attribute, data):\n",
    "    if pandas.isnull(sample[attribute]):\n",
    "        if attribute in NUMERICALS:\n",
    "            return data[attribute].median()\n",
    "        return int(attribute == 'GIERACE' )\n",
    "    elif attribute == 'FRAC':\n",
    "        return int(sample['FAANYSLD'] or sample['FAANYWST'] or sample['FAANYHIP'] or sample['XMDSQGE1'])\n",
    "    return sample[attribute]\n",
    "\n",
    "\n",
    "# Hyper parameter tuning\n",
    "def RandomSearch(estimator, modelName, params, Xtrain, ytrain, Xtest, ytest, score, verb=0):\n",
    "    t0 = time.time()\n",
    "    print('\\nSearching grid -', modelName, '(' + score + ')...\\n')\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    grid = RandomizedSearchCV(estimator, param_distributions=params, cv=cv, scoring=score, n_jobs=1, verbose=verb)  # n_jobs threads it if possible.\n",
    "    grid.fit(Xtrain, ytrain)\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "# calculate confidence interval from the scores\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * numpy.array(data)\n",
    "    m, se = numpy.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., len(a) - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "# Calculate the 95% CI for AUC in Random Forest\n",
    "# def conf_auc(test_predictions, ground_truth, bootstrap=1000, seed=None,  confint=0.95):\n",
    "#     \"\"\"Takes as input test predictions, ground truth, number of bootstraps, seed, and confidence interval\"\"\"\n",
    "#     #inspired by https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals by ogrisel\n",
    "#     bootstrapped_scores = []\n",
    "#     rng = numpy.random.RandomState(seed)\n",
    "#     if confint>1:\n",
    "#         confint=confint/100\n",
    "#     for i in range(bootstrap):\n",
    "#         # bootstrap by sampling with replacement on the prediction indices\n",
    "#         indices = rng.randint(0, len(test_predictions) - 1, len(test_predictions))\n",
    "#         if len(numpy.unique(ground_truth[indices])) < 2:\n",
    "#             continue\n",
    "\n",
    "#         score = metrics.roc_auc_score(ground_truth[indices], test_predictions[indices])\n",
    "#         bootstrapped_scores.append(score)\n",
    "    \n",
    "#     sorted_scores = numpy.array(bootstrapped_scores)\n",
    "#     sorted_scores.sort()\n",
    "\n",
    "#     lower_bound=(1-confint)/2\n",
    "#     upper_bound=1-lower_bound\n",
    "#     confidence_lower = sorted_scores[int(lower_bound * len(sorted_scores))]\n",
    "#     confidence_upper = sorted_scores[int(upper_bound * len(sorted_scores))]\n",
    "#     auc = metrics.roc_auc_score(ground_truth, test_predictions)\n",
    "#     print(\"{:0.0f}% confidence interval for the score: [{:0.3f} - {:0.3}] and your AUC is: {:0.3f}\".format(confint*100, confidence_lower, confidence_upper, auc))\n",
    "#     confidence_interval = (confidence_lower, auc, confidence_upper)\n",
    "#     return confidence_interval\n",
    "\n",
    "# Calculate the 95% CI for Accuracy in Random Forest\n",
    "# def conf_accu(test_predictions, ground_truth, bootstrap=1000, seed=None,  confint=0.95):\n",
    "#     \"\"\"Takes as input test predictions, ground truth, number of bootstraps, seed, and confidence interval\"\"\"\n",
    "#     #inspired by https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals by ogrisel\n",
    "#     bootstrapped_scores = []\n",
    "#     rng = np.random.RandomState(seed)\n",
    "#     if confint>1:\n",
    "#         confint=confint/100\n",
    "#     for i in range(bootstrap):\n",
    "#         # bootstrap by sampling with replacement on the prediction indices\n",
    "#         indices = rng.randint(0, len(test_predictions) - 1, len(test_predictions))\n",
    "#         if len(np.unique(ground_truth[indices])) < 2:\n",
    "#             continue\n",
    "        \n",
    "#         score = metrics.accuracy_score(ground_truth[indices], test_predictions[indices], normalize=False)\n",
    "#         bootstrapped_scores.append(score)\n",
    "    \n",
    "#     sorted_scores = np.array(bootstrapped_scores)\n",
    "#     sorted_scores.sort()\n",
    "\n",
    "#     lower_bound=(1-confint)/2\n",
    "#     upper_bound=1-lower_bound\n",
    "#     confidence_lower = sorted_scores[int(lower_bound * len(sorted_scores))]\n",
    "#     confidence_upper = sorted_scores[int(upper_bound * len(sorted_scores))]\n",
    "#     auc = metrics.accuracy_score(ground_truth, test_predictions[indices], normalize=False)\n",
    "#     print(\"{:0.0f}% confidence interval for the score: [{:0.3f} - {:0.3}] and your accuracy is: {:0.3f}\".format(confint*100, confidence_lower, confidence_upper, auc))\n",
    "#     confidence_interval = (confidence_lower, auc, confidence_upper)\n",
    "#     return confidence_interval\n",
    "\n",
    "def get_data(plot=True):\n",
    "    data = pandas.read_excel('mros_1103snps.xlsx')\n",
    "    # drop HA_SLDFXFU where only 10% is filled, drop subjectid,\n",
    "    data.drop(['HA_SLDFXFU', 'TURSMOKE', 'HA_SLDFX', 'HA_WRSTFX'], axis=1, inplace=True)\n",
    "\n",
    "    # make frac attribute as fracture and return the values from fill_empty_Cell with either FAANYSLD, FAANYWST or HIP\n",
    "    data['FRAC'] = 0\n",
    "    for attribute in data.keys():\n",
    "        data[attribute] = data.apply(lambda sample: fill_empty_cell(sample, attribute, data), axis=1)\n",
    "    # drop the other fractured values\n",
    "    data.drop(['FAANYSLD', 'FAANYWST'], axis=1, inplace=True)\n",
    "    # encode the categorical data\n",
    "    data = pandas.DataFrame(pandas.get_dummies(data, columns=['GIERACE', 'PHYS_MROS', 'NFWLKSPD']))\n",
    "    #features = list(data)[20:-6]\n",
    "    print(data.shape, ' SHAPE OF DAT')\n",
    "    # setting Y and X\n",
    "    Y = data['FRAC']\n",
    "    X = pandas.read_excel('ready_to_go.xlsx')\n",
    "    #X = data.drop(['SUBJECTID', 'HA_LSD', 'BUAMEAN', 'FAHIPFV1', 'FASLDFV1', 'FAWSTFV1', 'EFSTATUS',\n",
    "    #                'HA_BMI', 'FAANYHIP', 'HA_CALCIUM', 'XMDSQGE1', 'XMSQGE2', 'CLINIC', 'FRAC'], axis=1)\n",
    "\n",
    "    #feature_data = data[features]\n",
    "\n",
    "    #X.drop(features, axis=1, inplace=True)\n",
    "    # smote\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    sm = SMOTE(random_state=2, ratio = 1.0)\n",
    "    \n",
    "    x_train_s, y_train_s = sm.fit_resample(x_train, y_train)\n",
    "    # Xtrain, Xtest, ytrain, ytest = train_test_split(X_df, Y_df, test_size=0.2)\n",
    "    parameters = {\n",
    "        'n_estimators': [300, 800, 1000, 1200],\n",
    "        'max_features': ['auto', 'sqrt', 0.2],\n",
    "        'min_samples_split': [10, 20, 5, 2],\n",
    "        'max_depth': [2, 3, 5, 8],\n",
    "        'random_state': [45]\n",
    "    }\n",
    "    # split data for parameter sweep\n",
    "    gbr = RandomForestClassifier()\n",
    "    model = RandomSearch(estimator=gbr, modelName='Random Forest Classifier', params=parameters, Xtrain=x_train_s, ytrain=y_train_s, Xtest=x_test, ytest=y_test, score='roc_auc')\n",
    "    # model=RandomForestClassifier(n_estimators=800, max_depth=5)\n",
    "    model.fit(x_train_s, y_train_s)\n",
    "#     print(model.feature_importances_)\n",
    "    # ypred = model.predict(Xtest)\n",
    "    yscore_raw = model.predict_proba(x_test)\n",
    "    yscore = [s[1] for s in yscore_raw]\n",
    "    fpr, tpr, thresh = roc_curve(y_test, yscore)\n",
    "    print(confusion_matrix(y_test,model.predict(x_test)))\n",
    "    y_pred = model.fit(x_test, y_test)\n",
    "    print(list(X))\n",
    "#    conf_auc(y_pred, y_test)\n",
    "#    conf_accu(y_pred, y_test)\n",
    "#    print(confusion_matrix(ytest,yscore))\n",
    "    auc = roc_auc_score(y_test, yscore)\n",
    "    ytest = numpy.array(y_test)\n",
    "    \n",
    "    rf_final = pd.Series(model.predict(x_test))\n",
    "    rf_final.to_csv('rf_y_pred.csv')\n",
    "    # Receiver Operating Characteristic-\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(fpr, tpr, '--k', label='%s ROC (area = %0.2f)' % ('Random Forest', auc))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1],'-k')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "        plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "        plt.title('Random Forest')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig('random_forest_MOF.png')\n",
    "        plt.show()\n",
    "    return fpr, tpr, thresh, auc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature importance\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import scikitplot\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, brier_score_loss, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold  # http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fill numerical empty cells with median of the column, race with 1 (for white) and other categorical empty cells with the 0\n",
    "def fill_empty_cell(sample, attribute, data):\n",
    "    if pandas.isnull(sample[attribute]):\n",
    "        if attribute in NUMERICALS:\n",
    "            return data[attribute].median()\n",
    "        return int(attribute == 'GIERACE' )\n",
    "    elif attribute == 'FRAC':\n",
    "        return int(sample['FAANYSLD'] or sample['FAANYWST'] or sample['FAANYHIP'])\n",
    "    return sample[attribute]\n",
    "\n",
    "NUMERICALS = ['GIAGE1', 'HA_HEIGHT', 'HA_WEIGHT', 'TUDRPRWK', 'B1FND', 'GRS_FN', 'HA_SMOKE_0.0', 'HA_SMOKE_1.0', 'HA_SMOKE_2.0', 'GIERACE_1.0',\n",
    "              'GIERACE_2.0', 'GIERACE_3.0', 'GIERACE_4.0', 'GIERACE_5.0', 'CLINIC_1.0','CLINIC_2.0', 'CLINIC_3.0',  \n",
    "              'CLINIC_4.0', 'CLINIC_5.0', 'CLINIC_6.0','NFWLKSPD_0.0', 'NFWLKSPD_1.0', 'NFWLKSPD_2.0']\n",
    "\n",
    "# Hyper parameter tuning\n",
    "def RandomSearch(estimator, modelName, params, Xtrain, ytrain, Xtest, ytest, score, verb=0):\n",
    "    t0 = time.time()\n",
    "    print('\\nSearching grid -', modelName, '(' + score + ')...\\n')\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    grid = RandomizedSearchCV(estimator, param_distributions=params, cv=cv, scoring=score, n_jobs=1, verbose=verb)  # n_jobs threads it if possible.\n",
    "    grid.fit(Xtrain, ytrain)\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "data = pandas.read_excel('mros_1103snps.xlsx')\n",
    "# drop HA_SLDFXFU where only 10% is filled, drop subjectid,\n",
    "data.drop(['HA_SLDFXFU', 'TURSMOKE', 'HA_SLDFX', 'HA_WRSTFX'], axis=1, inplace=True)\n",
    "\n",
    "# make frac attribute as fracture and return the values from fill_empty_Cell with either FAANYSLD, FAANYWST or HIP\n",
    "data['FRAC'] = 0\n",
    "for attribute in data.keys():\n",
    "    data[attribute] = data.apply(lambda sample: fill_empty_cell(sample, attribute, data), axis=1)\n",
    "# drop the other fractured values\n",
    "data.drop(['FAANYSLD', 'FAANYWST'], axis=1, inplace=True)\n",
    "# encode the categorical data\n",
    "data = pandas.DataFrame(pandas.get_dummies(data, columns=['GIERACE', 'PHYS_MROS', 'NFWLKSPD']))\n",
    "#features = list(data)[20:-6]\n",
    "print(data.shape, ' SHAPE OF DAT')\n",
    "# setting Y and X\n",
    "Y = data['FRAC']\n",
    "X = pandas.read_excel('ready_to_go.xlsx')\n",
    "#X = data.drop(['SUBJECTID', 'HA_LSD', 'BUAMEAN', 'FAHIPFV1', 'FASLDFV1', 'FAWSTFV1', 'EFSTATUS',\n",
    "#                'HA_BMI', 'FAANYHIP', 'HA_CALCIUM', 'XMDSQGE1', 'XMSQGE2', 'CLINIC', 'FRAC'], axis=1)\n",
    "\n",
    "#feature_data = data[features]\n",
    "\n",
    "#X.drop(features, axis=1, inplace=True)\n",
    "# smote\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "sm = SMOTE(random_state=2, ratio = 1.0)\n",
    "    \n",
    "x_train_s, y_train_s = sm.fit_resample(x_train, y_train)\n",
    "# Xtrain, Xtest, ytrain, ytest = train_test_split(X_df, Y_df, test_size=0.2)\n",
    "\n",
    "parameters = {\n",
    "        'n_estimators': [300, 800, 1000, 1200],\n",
    "        'max_features': ['auto', 'sqrt', 0.2],\n",
    "        'min_samples_split': [10, 20, 5, 2],\n",
    "        'max_depth': [2, 3, 5, 8],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "# split data for parameter sweep\n",
    "gbr = RandomForestClassifier()\n",
    "model = RandomSearch(estimator=gbr, modelName='Random Forest Classifier', params=parameters, Xtrain=x_train_s, ytrain=y_train_s, Xtest=x_test, ytest=y_test, score='roc_auc')\n",
    "print(model.feature_importances_)\n",
    "    \n",
    "feature_impor = pd.Series(model.feature_importances_)\n",
    "feature_impor.to_csv('feature_impor.csv')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import scikitplot\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold  # http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from sksurv.nonparametric import kaplan_meier_estimator\n",
    "except ImportError:\n",
    "    kaplan_meier_estimator = None\n",
    "\n",
    "NUMERICALS = ['GIAGE1', 'HA_HEIGHT', 'HA_WEIGHT', 'TUDRPRWK', 'B1FND', 'HA_SMOKE_0.0', 'HA_SMOKE_1.0', 'HA_SMOKE_2.0', 'GIERACE_1.0',\n",
    "              'GIERACE_2.0', 'GIERACE_3.0', 'GIERACE_4.0', 'GIERACE_5.0', 'CLINIC_1.0','CLINIC_2.0', 'CLINIC_3.0',  \n",
    "              'CLINIC_4.0', 'CLINIC_5.0', 'CLINIC_6.0','NFWLKSPD_0.0', 'NFWLKSPD_1.0', 'NFWLKSPD_2.0', 'BUAMEAN', 'QUIMEAN', 'SOSMEAN', 'PHYS_MROS_1.0', 'PHYS_MROS_2.0', 'PHYS_MROS_3.0']\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def fill_empty_cell(sample, attribute, data):\n",
    "    # numericals = ['B1FND', 'B1THD', 'B1TLD', 'BUAMEAN', 'GIAGE1', 'HA_BMI', 'HA_CALCIUM', 'HA_HEIGHT', 'HA_LSD', 'HA_WEIGHT', 'TUDRPRWK']\n",
    "    if pandas.isnull(sample[attribute]):\n",
    "        return data[attribute].median() if attribute in NUMERICALS else int(attribute == 'GIERACE')\n",
    "    elif attribute == 'FRAC':\n",
    "        return int(sample['FAANYSLD'] or sample['FAANYWST'] or sample['FAANYHIP'] or sample['XMDSQGE1'])\n",
    "    elif attribute == 'STATUS':\n",
    "        return int(sample['EFSTATUS'] > 0) or int(sample['FAANYSLD'] or sample['FAANYWST'] or sample['FAANYHIP'])\n",
    "    elif attribute == 'DAYS':\n",
    "        return min(sample['FAHIPFV1'], sample['FASLDFV1'], sample['FAWSTFV1'])\n",
    "    else:\n",
    "        return sample[attribute]\n",
    "\n",
    "# Hyper parameter tuning\n",
    "def RandomSearch(estimator, modelName, params, Xtrain, ytrain, Xtest, ytest, score, verb=0):\n",
    "    t0 = datetime.now()\n",
    "    print('\\nSearching grid -', modelName, '(' + score + ')...\\n')\n",
    "    grid = RandomizedSearchCV(estimator, param_distributions=params, cv=KFold(n_splits=3, shuffle=True), scoring=score, n_jobs=1, verbose=verb)  # n_jobs threads it if possible.\n",
    "    grid.fit(Xtrain, ytrain)\n",
    "    print('The best parameters are\\n', grid.best_params_)\n",
    "    print('The best ' + score + ' score is %0.4f \\n' % (grid.best_score_ * 100))\n",
    "    ypred = grid.predict(Xtest)\n",
    "    # cm = confusion_matrix(ytest, ypred)\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    # pyplot.figure()\n",
    "    # plot_confusion_matrix(cm, classes=[0, 1], normalize=False, title='Normalized confusion matrix')\n",
    "    # pyplot.show()\n",
    "    print('\\n', (datetime.now() - t0).total_seconds(), 'seconds')\n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "# calculate confidence interval from the scores\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * numpy.array(data)\n",
    "    n = len(a)\n",
    "    m, se = numpy.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "\n",
    "def get_data(plot=True):\n",
    "    data = pandas.read_excel('mros_1103snps.xlsx')\n",
    "    # drop HA_SLDFXFU where only 10% is filled, drop subjectid,\n",
    "    data.drop(['HA_SLDFXFU', 'TURSMOKE', 'HA_SLDFX', 'HA_WRSTFX'], axis=1, inplace=True)\n",
    "\n",
    "    # make frac attribute as fracture and return the values from fill_empty_Cell with either FAANYSLD, FAANYWST or HIP\n",
    "    data['FRAC'] = 0\n",
    "    for attribute in data.keys():\n",
    "        data[attribute] = data.apply(lambda sample: fill_empty_cell(sample, attribute, data), axis=1)\n",
    "    # drop the other fractured values\n",
    "    data.drop(['FAANYSLD', 'FAANYWST'], axis=1, inplace=True)\n",
    "    # encode the categorical data\n",
    "    data = pandas.DataFrame(pandas.get_dummies(data, columns=['GIERACE', 'PHYS_MROS', 'NFWLKSPD']))\n",
    "    #features = list(data)[20:-6]\n",
    "    print(data.shape, ' SHAPE OF Data')\n",
    "    # setting Y and X\n",
    "    Y = data['FRAC']\n",
    "    X = pandas.read_excel('ready_to_go.xlsx')    \n",
    "    #X = data.drop(['SUBJECTID', 'HA_LSD', 'BUAMEAN', 'FAHIPFV1', 'FASLDFV1', 'FAWSTFV1', 'EFSTATUS',\n",
    "    #               'HA_BMI', 'FAANYHIP', 'HA_CALCIUM', 'XMDSQGE1', 'XMSQGE2', 'CLINIC', 'FRAC'], axis=1)\n",
    "\n",
    "    # X_df['GRS_LS'] = feature_data.dot(weight_LS)\n",
    "    # X_df['GRS_FN'] = feature_data.dot(weight_FN)\n",
    "    # X.drop(features, axis=1, inplace=True)\n",
    "    # smote\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    sm = SMOTE(random_state=2, ratio = 1.0)\n",
    "    x_train_s, y_train_s = sm.fit_resample(x_train, y_train)\n",
    "  \n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    parameters = {\n",
    "        'loss': ['deviance', 'exponential'],\n",
    "        'n_estimators': [200, 500, 800, 1000, 1200],\n",
    "        'learning_rate': [0.001, 0.003, 0.005],\n",
    "        'subsample': [0.3, 0.5, 0.7, 0.9],  # <1.0 results in reduction of variance and increase in bias\n",
    "        'min_samples_split': [2, 5, 8],\n",
    "        'max_features': ['auto', 'log2', 'sqrt', 0.2],\n",
    "        'random_state': [43],\n",
    "        'max_depth': [2, 3, 5],\n",
    "        'min_impurity_decrease': [0.15, 0.1, 0.08, 0.05]\n",
    "    }\n",
    "    # split data for parameter sweep\n",
    "    model = RandomSearch(estimator=GradientBoostingClassifier(), modelName='Gradient Boosting Classifier', params=parameters, Xtrain=x_train_s, ytrain=y_train_s, Xtest=x_test, ytest=y_test, score='roc_auc')\n",
    "    # model=GradientBoostingClassifier(subsample=0.3, n_estimators=800, min_samples_split=2, min_impurity_decrease=0.05, max_features='sqrt', max_depth=3, loss='deviance', learning_rate=0.01)\n",
    "    model.fit(x_train_s, y_train_s)\n",
    "    print(model.feature_importances_)\n",
    "    # ypred = model.predict(Xtest)\n",
    "    yscore_raw = model.predict_proba(x_test)\n",
    "    yscore = [s[1] for s in yscore_raw]\n",
    "    fpr, tpr, thresh = roc_curve(y_test, yscore)\n",
    "    auc = roc_auc_score(y_test, yscore)\n",
    "    ytest = numpy.array(y_test)\n",
    "    print(confusion_matrix(y_test,model.predict(x_test)))\n",
    "    \n",
    "    print(list(X))\n",
    "\n",
    "    \n",
    "    gb_final = pd.Series(model.predict(x_test))\n",
    "    gb_final.to_csv('gb_y_pred.csv')\n",
    "    \n",
    "#     gb_y_test = pd.Series(y_test)\n",
    "#     gb_y_test.to_csv('gb_y_test.csv')\n",
    "    \n",
    "    \n",
    "    # Receiver Operating Characteristic-\n",
    "    # plot roc curve\n",
    "    if plot:# Custom settings for the plot\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(fpr, tpr, ',-.k', label='%s ROC (area = %0.2f)' % ('Gradient Boosting', auc))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1],'-k')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "        plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "        plt.title('Gradient Boosting')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig('gradient_boosting_MOF.png')\n",
    "        plt.show()   # Display\n",
    "\n",
    "    return fpr, tpr, thresh, auc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('datamros1103', 'rb') as file_handler:\n",
    "    data = pickle.load(file_handler)\n",
    "    X, Y = data.get('X', []).values, data.get('Y', []).values\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 10\n",
    "#\n",
    "# # The below is necessary for starting Numpy generated random numbers\n",
    "# # in a well-defined initial state.\n",
    "np.random.seed(seed)\n",
    "# # The below is necessary for starting core Python generated random numbers\n",
    "# # in a well-defined state.\n",
    "rn.seed(seed)\n",
    "\n",
    "# according to keras documentation, numpy seed should be set before importing keras\n",
    "# information regarding setup for obtaining reproducible results using Keras during development in the following link\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "#tf.set_random_seed(seed)\n",
    "# Y = label_binarize(Y, classes=[0,1])\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "optimizer = 'adamax'\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "number_of_data = X.shape[0]\n",
    "number_of_train_data = int(.8*number_of_data)\n",
    "number_of_test_data = number_of_data-number_of_train_data\n",
    "\n",
    "# load dataset\n",
    "x_train, x_test = X[:number_of_train_data, :], X[number_of_train_data:, :]\n",
    "#mean_train_data = np.mean(train_data, axis=0)\n",
    "#std_train_data = np.std(train_data, axis=0)\n",
    "#x_train = (train_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "#x_test = (test_data - mean_train_data) / std_train_data  # mean variance normalization\n",
    "y_train, y_test = Y[:number_of_train_data], Y[number_of_train_data:]\n",
    "#x_test, y_test = sm.fit_resample(X,Y)\n",
    "\n",
    "# X_df, Xtest, Y_df, ytest = train_test_split(X_df, Y_df, test_size=0.2)\n",
    "# Xtrain, ytrain = sm.fit_resample(X_df, Y_df)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "# y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(20, kernel_initializer='normal', activation='sigmoid'))\n",
    "    #model.add(Dense(10, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
    "    #model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    # model = Sequential()\n",
    "    # model.add(Dense(13, input_dim=19, kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    # model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # # Compile model\n",
    "    # model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # return model\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "sm = SMOTE(random_state=2, ratio = 1.0)\n",
    "x_train_s, y_train_s = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "model = create_model()\n",
    "model.fit(x_train_s, y_train_s, epochs=100, batch_size=batch_size, verbose=1)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(y_pred, ' Predicted Y')\n",
    "\n",
    "# y_pred = model.predict(x_test)\n",
    "#y_pred = y_pred.astype('int32')\n",
    "#print(y_pred, ' YPOST')\n",
    "yscore_raw = model.predict_proba(x_test)\n",
    "yscore = [s[0] for s in yscore_raw]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yscore)\n",
    "#final = [(lambda i: 0 if i <= np.average(thresholds) else 1)(i) for i in y_pred]\n",
    "final = [(lambda i: 0 if i <= np.average(thresholds) else 1)(i) for i in y_pred]\n",
    "print(confusion_matrix(y_test, final))\n",
    "\n",
    "mlp_final = pd.Series(final)\n",
    "mlp_final.to_csv('mlp_y_pred.csv')\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, yscore)\n",
    "plt.plot(fpr, tpr, ':k', label='%s ROC (area = %0.2f)' % ('Neural Network', auc))\n",
    "\n",
    "# Calculate Area under the curve to display on the plot\n",
    "\n",
    "# Now, plot the computed values\n",
    "\n",
    "# Custom settings for the plot\n",
    "# Receiver Operating Characteristic-\n",
    "plt.plot([0, 1], [0, 1],'-k')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Neural Network')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('mlp_MOF.png')\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
